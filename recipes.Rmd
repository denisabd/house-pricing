---
title: "House Pricing Recipe"
author: "Denis Abdullin"
date: "`r Sys.Date()`"
output: 
  html_document:
  toc: true
---

Recipe with all steps in data pipeline.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


```{r, message=FALSE}
library(data.table)
library(tidyverse)
library(lubridate)
library(scales)
library(corrplot)
library(DT)
library(daml)
library(DataExplorer)
library(plotly)
library(GGally)
library(knitr)
library(kernlab)
```

```{r}
train <- as.data.frame(fread('house_price_train.csv', stringsAsFactors=TRUE))
test <- as.data.frame(fread('house_price_test.csv', stringsAsFactors=TRUE))
```


Full recipe
```{r}

train <- train %>%
  filter(GrLivArea < 4500)

recipe <- recipe(train) %>%
  #set roles
  update_role(everything(), new_role = "predictor") %>%
  update_role(SalePrice, new_role = "outcome") %>%
  update_role(Id, new_role = "ID variable") %>%
  #data cleansing
  step_mutate(Id = as.character(Id)) %>%
  step_mutate(MSSubClass = as.factor(MSSubClass)) %>%
  step_mutate(GarageCars = as.character(GarageCars)) %>%
  step_mutate(GarageCars = ifelse((GarageCars == "4" | GarageCars == "3"), "3+", GarageCars)) %>%
  step_mutate(GarageCars = as.factor(GarageCars)) %>%
  #step_filter(GrLivArea < 4500) %>%
  #remove feautues
  step_rm(PoolQC, PoolArea, LandSlope, MiscFeature, Street, Utilities, Condition2, RoofMatl, Heating) %>%
  step_unknown(all_predictors(), -all_numeric(), new_level = "NA") %>% #impute missing values for categorical features
  step_log(all_outcomes()) %>%
  step_nzv(all_predictors(),-all_numeric()) %>%
  step_other(all_predictors(), -all_numeric(), threshold = 0.01) %>%
  step_novel(all_predictors(),-all_numeric(), new_level = "new") %>%
  step_lencode_mixed(all_predictors(), -all_numeric(), outcome = vars(SalePrice)) %>%
  step_knnimpute(all_predictors()) %>% #impute missing values
  step_corr(all_predictors(), threshold = 0.8) %>%
  step_normalize(all_predictors())

recipe

```

Now lets try **gbm** model
```{r message=FALSE, warning=FALSE}
# Tuning parameters for method 'gbm'
tuneGrid <- expand.grid(n.trees = 200,
                        interaction.depth = 7,
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

set.seed(123)

# Tuning parameters for method 'ranger'
gbm <- daml_train(train, recipe,
                  model = "gbm",
                  tracking = "mlflow",
                  feature_selection = "varimp",
                  max_features = 24,
                  p_subset = 0.3,
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  grid = tuneGrid)

gbm
varImp(gbm)


```

```{r message=FALSE, warning=FALSE}
submission <- test %>%
  daml_predict(model = gbm, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission, "gbm_baseline.csv", row.names = F)
```   


Recipe with PCA
```{r}
recipe_pca <- recipe(train) %>%
  #set roles
  update_role(everything(), new_role = "predictor") %>%
  update_role(SalePrice, new_role = "outcome") %>%
  update_role(Id, new_role = "ID variable") %>%
  #data cleansing
  step_mutate(Id = as.character(Id)) %>%
  step_mutate(MSSubClass = as.factor(MSSubClass)) %>%
  step_mutate(GarageCars = as.character(GarageCars)) %>%
  step_mutate(GarageCars = ifelse((GarageCars == "4" | GarageCars == "3"), "3+", GarageCars)) %>%
  step_mutate(GarageCars = as.factor(GarageCars)) %>%
  #remove feautues
  step_rm(PoolQC, PoolArea, LandSlope, MiscFeature, Street, Utilities, Condition2, RoofMatl, Heating) %>%
  step_unknown(all_predictors(), -all_numeric(), new_level = "NA") %>% #impute missing values for categorical features
  step_log(all_outcomes()) %>%
  step_nzv(all_predictors(),-all_numeric()) %>%
  step_other(all_predictors(), -all_numeric(), threshold = 0.01) %>%
  #step_lencode_mixed(all_predictors(), -all_numeric(), outcome = vars(SalePrice)) %>%
  step_dummy(all_predictors(), -all_numeric()) %>%
  step_knnimpute(all_predictors()) %>% #impute missing values
  step_corr(all_predictors(), threshold = 0.8) %>%
  step_pca(all_predictors(), num_comp = 400) %>%
  step_normalize(all_predictors())

```


Model with PCA
```{r}

# Tuning parameters for method 'ranger'
gbm_pca <- daml_train(train, recipe_pca,
                  model = "gbm",
                  tracking = "mlflow",
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  grid = tuneGrid)

varImp(gbm_pca)
```


```{r message=FALSE, warning=FALSE}
submission_pca <- test %>%
  daml_predict(model = gbm_pca, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission_pca, "gbm_pca.csv", row.names = F)
```


Recipe to select most important features
```{r}
recipe_fs <- recipe(train) %>%
  #set roles
  update_role(everything(), new_role = "predictor") %>%
  update_role(SalePrice, new_role = "outcome") %>%
  update_role(Id, new_role = "ID variable") %>%
  #data cleansing
  step_mutate(Id = as.character(Id)) %>%
  step_mutate(MSSubClass = as.factor(MSSubClass)) %>%
  step_mutate(GarageCars = as.character(GarageCars)) %>%
  step_mutate(GarageCars = ifelse((GarageCars == "4" | GarageCars == "3"), "3+", GarageCars)) %>%
  step_mutate(GarageCars = as.factor(GarageCars)) %>%
  #remove feautues
  step_rm(PoolQC, PoolArea, LandSlope, MiscFeature, Street, Utilities, Condition2, RoofMatl, Heating) %>%
  step_unknown(all_predictors(), -all_numeric(), new_level = "NA") %>% #impute missing values for categorical features
  step_log(all_outcomes()) %>%
  step_nzv(all_predictors(),-all_numeric()) %>%
  step_other(all_predictors(), -all_numeric(), threshold = 0.01) %>%
  step_novel(all_predictors(),-all_numeric(), new_level = "new") %>%
  step_lencode_mixed(all_predictors(), -all_numeric(), outcome = vars(SalePrice)) %>%
  step_knnimpute(all_predictors()) %>% #impute missing values
  step_YeoJohnson(all_predictors())

```

Now lets try **gbm** with **recipe**
```{r message=FALSE, warning=FALSE}

gbm_fs <- daml_train(train, 
                  recipe_fs,
                  model = "gbm",
                  tracking = "mlflow",
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  tunelen = 10)

gbm_fs

varimp <- varImp(gbm_fs)
varimp <- varimp$importance
varimp <- varimp %>%
  rownames_to_column(var = "feature") %>%
  arrange(desc(Overall)) %>%
  top_n(24)

features <- as.character(varimp$feature)


```

Submission scored really well on Kaggle
```{r message=FALSE, warning=FALSE}
submission_fs <- test %>%
  daml_predict(model = gbm_fs, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission_fs, "gbm_fs.csv", row.names = F)
```   

Feature interactions could be a very expensive computation. Lest create all feature combinations and then try to select the best features out of those.
```{r}

features

recipe_inter <- recipe(train) %>%
  #set roles
  update_role(everything(), new_role = "other") %>%
  update_role(features, new_role = "predictor") %>%
  update_role(SalePrice, new_role = "outcome") %>%
  update_role(Id, new_role = "ID variable") %>%
  #data cleansing
  step_mutate(Id = as.character(Id)) %>%
  step_mutate(MSSubClass = as.factor(MSSubClass)) %>%
  step_mutate(GarageCars = as.character(GarageCars)) %>%
  step_mutate(GarageCars = ifelse((GarageCars == "4" | GarageCars == "3"), "3+", GarageCars)) %>%
  step_mutate(GarageCars = as.factor(GarageCars)) %>%
  #remove feautues
  step_rm(PoolQC, PoolArea, LandSlope, MiscFeature, Street, Utilities, Condition2, RoofMatl, Heating) %>%
  step_unknown(all_predictors(), -all_numeric(), new_level = "NA") %>% #impute missing values for categorical features
  step_log(all_outcomes()) %>%
  step_nzv(all_predictors(),-all_numeric()) %>%
  step_other(all_predictors(), -all_numeric(), threshold = 0.01) %>%
  step_novel(all_predictors(),-all_numeric(), new_level = "new") %>%
  step_lencode_mixed(all_predictors(), -all_numeric(), outcome = vars(SalePrice)) %>%
  step_knnimpute(all_predictors()) %>% #impute missing values
  step_interact(~ all_predictors():all_predictors()) %>% #interaction
  step_YeoJohnson(all_predictors())


```

```{r message=FALSE, warning=FALSE}
# Tuning parameters for method 'gbm'
tuneGrid <- expand.grid(n.trees = 100,
                        interaction.depth = 7,
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

gbm_inter <- daml_train(train, 
                  recipe_inter,
                  model = "gbm",
                  tracking = "mlflow",
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  grid = tuneGrid)
                  #tunelen = 10)

gbm_inter

varImp(gbm_inter)

# now lets get most important features from interactions
varimp <- varImp(gbm_inter)
varimp <- varimp$importance
varimp <- varimp %>%
  rownames_to_column(var = "feature") %>%
  arrange(desc(Overall)) %>%
  top_n(24)

features_inter <- as.character(varimp$feature)

```


```{r}

prep_inter <- prep(recipe_inter, training = train)
bake_inter <- bake(prep_inter, train)


p <- ggplot(aes(x = SalePrice, y = OverallQual_x_GarageType), data = bake_inter) + 
    geom_point(color='blue') 
ggplotly(p)

#OverallQual:GrLivArea)
#TotalBsmtSF:GrLivArea)

```

```{r message=FALSE, warning=FALSE}
submission_inter <- test %>%
  daml_predict(model = gbm_inter, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission_inter, "gbm_inter.csv", row.names = F)
```   


Now when we have top 24 features including interactions, lets tune hyperparameters as well.
```{r}

features_inter

recipe_final <- recipe(train) %>%
  #set roles
  update_role(everything(), new_role = "other") %>%
  update_role(features, new_role = "predictor") %>%
  update_role(SalePrice, new_role = "outcome") %>%
  update_role(Id, new_role = "ID variable") %>%
  #data cleansing
  step_mutate(Id = as.character(Id)) %>%
  step_mutate(MSSubClass = as.factor(MSSubClass)) %>%
  step_mutate(GarageCars = as.character(GarageCars)) %>%
  step_mutate(GarageCars = ifelse((GarageCars == "4" | GarageCars == "3"), "3+", GarageCars)) %>%
  step_mutate(GarageCars = as.factor(GarageCars)) %>%
  #remove feautues
  step_rm(PoolQC, PoolArea, LandSlope, MiscFeature, Street, Utilities, Condition2, RoofMatl, Heating) %>%
  step_unknown(all_predictors(), -all_numeric(), new_level = "NA") %>% #impute missing values for categorical features
  step_log(all_outcomes()) %>%
  step_nzv(all_predictors(),-all_numeric()) %>%
  step_other(all_predictors(), -all_numeric(), threshold = 0.01) %>%
  step_novel(all_predictors(),-all_numeric(), new_level = "new") %>%
  step_lencode_mixed(all_predictors(), -all_numeric(), outcome = vars(SalePrice)) %>%
  step_knnimpute(all_predictors()) %>% #impute missing values
  step_interact(~ OverallQual:GrLivArea) %>%
  step_interact(~ TotalBsmtSF:GrLivArea) %>%
  step_YeoJohnson(all_predictors())

recipe_final

```

```{r message=FALSE, warning=FALSE}
# Tuning parameters for method 'gbm'
tuneGrid <- expand.grid(n.trees = c(100,150,200),
                        interaction.depth = c(2,3,4,5,7,9),
                        shrinkage = 0.1,
                        n.minobsinnode = c(5,10,15))

gbm_final <- daml_train(train,
                        recipe_final,
                        model = "gbm",
                        tracking = "mlflow",
                        tracking_uri = "http://localhost:5000",
                        mlflow_experiment = "house pricing",
                        grid = tuneGrid)


gbm_final
varImp(gbm_final)


```

```{r message=FALSE, warning=FALSE}
submission_final <- test %>%
  daml_predict(model = gbm_final, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission_final, "gbm_final.csv", row.names = F)
```   

svm Model
```{r message=FALSE, warning=FALSE}
svm <- daml_train(train,
                  recipe_final,
                  model = "svmLinear",
                  tracking = "mlflow",
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  tunelen = 10)


svm
varImp(svm)


```

```{r message=FALSE, warning=FALSE}
submission_svm <- test %>%
  daml_predict(model = svm, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission_svm, "svm.csv", row.names = F)
```   
 
 
RF Model
```{r message=FALSE, warning=FALSE}

rf <- daml_train(train,
                  recipe_final,
                  model = "rf",
                  tracking = "mlflow",
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  tunelen = 5)


rf
varImp(rf)


```

```{r message=FALSE, warning=FALSE}
submission_rf <- test %>%
  daml_predict(model = rf, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission_rf, "rf.csv", row.names = F)
```   


KNN Model
```{r message=FALSE, warning=FALSE}

knn <- daml_train(train,
                  recipe_final,
                  model = "knn",
                  tracking = "mlflow",
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  tunelen = 25)


knn
varImp(knn)


```

```{r message=FALSE, warning=FALSE}
submission_knn <- test %>%
  daml_predict(model = knn, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission_knn, "knn.csv", row.names = F)
```  

ridge Model
```{r message=FALSE, warning=FALSE}

ridge <- daml_train(train,
                  recipe_final,
                  model = "ridge",
                  tracking = "mlflow",
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  tunelen = 10)


ridge
varImp(ridge)


```

```{r message=FALSE, warning=FALSE}
submission_ridge <- test %>%
  daml_predict(model = ridge, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission_ridge, "ridge.csv", row.names = F)
```  


Ensemble Model
```{r}
submission_gbm <- submission_fs %>%
  dplyr::rename(SalePriceGBM = SalePrice)

submission_svm <- submission_svm %>%
  dplyr::rename(SalePriceSVM = SalePrice)

submission_ridge <- submission_ridge %>%
  dplyr::rename(SalePriceRidge = SalePrice)

submission_rf <- submission_rf %>%
  dplyr::rename(SalePriceRF = SalePrice)

submission_knn <- submission_knn %>%
  dplyr::rename(SalePriceKNN = SalePrice)

submission_ensemble <- submission_gbm %>%
  inner_join(submission_rf, by = 'Id') %>%
  inner_join(submission_ridge, by = 'Id') %>%
  inner_join(submission_knn, by = 'Id') %>%
  inner_join(submission_svm, by = 'Id') %>%
  mutate(SalePrice = 0.75*SalePriceGBM + 0.1*SalePriceSVM + 0.15*SalePriceRidge) %>%
  select(Id, SalePrice)


write.csv(submission_ensemble,"ensemble.csv", row.names = F)

```

**THIS SCORES 0.12357 ON KAGGLE**
