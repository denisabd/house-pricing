---
title: "House Pricing EDA"
author: "Denis Abdullin"
date: "`r Sys.Date()`"
output: 
  html_document:
  toc: true
---

This is an extremely rich data set with features that are not anonymized, so it makes an excellent playground for exploratory data analysis.

Training Data

* [Missing Data](#missing)
* [Target](#target)
* [Investigate Features](#features)
* [Correlation](#correlation)
* [Numeric Features](#numfeatures)
* [Variable Importance](#varimp)
* [Categorical Features](#catfeatures)
* [Model](#model)


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


```{r, message=FALSE}
library(data.table)
library(tidyverse)
library(lubridate)
library(scales)
library(corrplot)
library(DT)
library(daml)
library(plotly)
library(GGally)
library(knitr)

```

# Training Data

```{r}
train <- as.data.frame(fread('house_price_train.csv', stringsAsFactors=TRUE))
test <- as.data.frame(fread('house_price_test.csv', stringsAsFactors=TRUE))
```

## Investigate features {#features}  
Some variables can appear to be numeric, but actually are factors: **MSSubClass**

```{r}
# numeric
train %>%
  select(MSSubClass) %>%
  str()

train <- train %>%
  mutate(MSSubClass = as.factor(MSSubClass))

test <- test %>%
  mutate(MSSubClass = as.factor(MSSubClass))

# actually have 9-15 levels and are factors
train %>%
  select(MSSubClass) %>%
  str()

```

There are features that show us year of construction/reconstruction.
``` {r}
p <- ggplot(aes(x = YearBuilt, y = SalePrice), data = train) + 
    geom_point(color='blue') 

ggplotly(p)
```

``` {r}
p <- ggplot(aes(x = GarageYrBlt, y = SalePrice), data = train) + 
    geom_point(color='blue') 

ggplotly(p)
```


Id variable should be converted to character and also lets check correlation between **YearBuilt** and **GarageYrBlt** 
``` {r}
train %>%
  select(contains("year"), contains("yr"), Id) %>%
  str()

train <- train %>%
  mutate(Id = as.character(Id))

test <- test %>%
  mutate(Id = as.character(Id))

cor(train$YearBuilt,train$GarageYrBlt,use="complete.obs")

```

YearRemodAdd
``` {r}
p <- ggplot(aes(x = YearRemodAdd, y = SalePrice), data = train) + 
    geom_point(color='blue') 

cor(train$YearRemodAdd,train$SalePrice,use="complete.obs")

ggplotly(p)
```

YrSold appears to be a bad predictor, we can remove it.
``` {r}
p <- ggplot(aes(x = YrSold, y = SalePrice), data = train) + 
    geom_point(color='blue') 

cor(train$YrSold,train$SalePrice,use="complete.obs")

ggplotly(p)
```

## Missing Data {#missing}

How much data is missing?

```{r}
miss_pct <- map_dbl(train, function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })

miss_pct <- miss_pct[miss_pct > 0]

p <- data.frame(miss=miss_pct, var=names(miss_pct), row.names=NULL) %>%
      ggplot(aes(x=reorder(var, -miss), y=miss)) + 
      geom_bar(stat='identity', fill='red') +
      labs(x='', y='% missing', title='Percent missing data by feature') +
      theme(axis.text.x=element_text(angle=90, hjust=1))

ggplotly(p)
names(train)

```

Lets take a look at features with most missing values. <br />
We have to think about assigning values instad of NAs or dropping the feature.
``` {r}
train %>%
  select(PoolQC, MiscFeature,Alley,Fence,FireplaceQu,LotFrontage) %>%
  summary()

```


PoolArea
```{r}
train %>%
  select(PoolArea) %>%
  summary()

poolarea <- as.data.frame(train$PoolArea)
summary(train$PoolArea)

```
Features like **PoolQC** and **MiscFeature** have a very low number of meaningful values.  <br />
We will drop those two, for other features we will replace **NA** with a value like **None**.

``` {r}
train <- train %>%
  select(-PoolQC,-MiscFeature)

cat_columns <- train %>%
  mutate_if(is.factor,as.character) %>%
  select_if(is.character) %>%
  names()

train_cat <- train %>%
  select(cat_columns) %>%
  mutate_if(is.factor,as.character) %>%
  replace(., is.na(.), "None") %>%
  mutate_if(is.character,as.factor)

train_num <- train %>%
  select(-cat_columns)

train <- bind_cols(train_num,train_cat)

test <- test %>%
  select(-PoolQC,-MiscFeature)

test_cat <- test %>%
  select(cat_columns) %>%
  mutate_if(is.factor,as.character) %>%
  replace(., is.na(.), "None") %>%
  mutate_if(is.character,as.factor)

test_num <- test %>%
  select(-cat_columns)

test <- bind_cols(test_num,test_cat)

miss_pct <- map_dbl(train, function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })

miss_pct <- miss_pct[miss_pct > 0]

p <- data.frame(miss=miss_pct, var=names(miss_pct), row.names=NULL) %>%
      ggplot(aes(x=reorder(var, -miss), y=miss)) + 
      geom_bar(stat='identity', fill='red') +
      labs(x='', y='% missing', title='Percent missing data by feature') +
      theme(axis.text.x=element_text(angle=90, hjust=1))

ggplotly(p)
  
```

## Target {#target}
Lets check a histogram for our target - **SalePrice**:
``` {r}
p <- ggplot(aes(x = SalePrice), data = train) + 
          geom_histogram(fill='red',bins = 50, color = "black",aes(y=..density..)) + 
          geom_density(aes(y=..density..), color = "blue", size = 0.5) +
          ggtitle('SalePrice Histogram') 

ggplotly(p)

```

Target isn't normally distributed, log transformation solves the problem:
``` {r}
p <- ggplot(aes(x = log(SalePrice)), data = train) + 
          geom_histogram(fill='red',bins = 50, color = "black",aes(y=..density..)) + 
          geom_density(aes(y=..density..), color = "blue", size = 0.5) +
          ggtitle('Log SalePrice Histogram') 

ggplotly(p)

```

## Correlation {#correlation}
Lets take a look at the correlation matrix for numeric predictors
```{r}

# variables that are internal to home
num_columns <- train %>%
  select_if(is.numeric) %>%
  names()

print(num_columns)

cor <- cor(train[, num_columns], use="complete.obs")

cor_list <- as.data.frame(cor) %>%
            rownames_to_column("Feature") %>%
            select(Feature, SalePrice) %>%
            filter(Feature != "SalePrice") %>%
            arrange(desc(abs(SalePrice)))

cor_list

```


Lets plot top 10 features in terms of absolute value of correlation with SalePrice
```{r}

cor_10 <- cor_list %>%
  top_n(10) %>%
  select(Feature) %>%
  as.matrix() %>%
  as.character()

cormatrix_10 <- cor(train[, c(cor_10,"SalePrice")], use="complete.obs")

corrplot::corrplot(cormatrix_10, method = "color",
               type = "upper", order = "hclust", number.cex = .7,
               addCoef.col = "black", # Add coefficient of correlation
               tl.col = "black", tl.srt = 90, # Text label color and rotation
               sig.level = 0.05, insig = "blank", # Combine with significance
               diag = FALSE)# hide correlation coefficient on the principal diagonal


```


**OverallQual** and **GrLivArea** are strongly correlated with **SalePrice** <br />
There are twin features that are correlated to each other: <br />
**GarageCars** and **GarageArea** have an obvious realtion - if we have bigger garage, more cars fit. <br />
**TotalBsmtSF** and **1stFlrSF** represent the square footage of basement and first floor respectively. Values are logically very close to each other.  <br />
**GrLivArea** and **TotRmsAbvGrd** - more space generally means more rooms. <br />
We can tackle correlation issues by excluding correlated variables in our machine learning pipeline with **recipe** setting correlation threshold to 0.8 <br />

## Numeric Features {#numfeatures}
Correlation matrix shows that **GrLivArea** is highly correlated with **SalePrice** as one might expect
```{r}

p <- ggplot(aes(x = GrLivArea, y = SalePrice), data = train) + 
     geom_point(color='red') +
     geom_smooth(method='lm', formula= y~x)

ggplotly(p)

```

There is a couple of records that seem like outliers with  **GrLivArea** higher than 4500 SF. <br />
There are also 2 record in top right, but altough being away from the group those seem to follow the general pattern.

```{r}

train <- train %>%
  filter(GrLivArea < 4500)

```


**GarageArea** & **SalePrice**:
```{r}

p <- ggplot(aes(x = GarageArea, y = SalePrice), data = train) + 
     geom_point(color='orange') +
     geom_smooth(method='lm', formula= y~x)

ggplotly(p)

```


**TotalBsmtSF** & **SalePrice**:
```{r}
p <- ggplot(aes(x = TotalBsmtSF, y = SalePrice), data = train) + 
    geom_point(color='violet') +
    geom_smooth(method='lm', formula= y~x)

ggplotly(p)

```


Distribution of **GarageCars**:
```{r}
p <- ggplot(aes(x = GarageCars), data = train) + 
          geom_histogram(fill='red', bins = 5) + 
          ggtitle('Distribution of garage cars count')

ggplotly(p)

```


There are only 5 properties with 4 **GarageCars**, lets groupd them with ones that have 3 and rename into 3+ together with changing data type to factor. <br />
Also we will update vector of numeric and categorical columns.
```{r}
train <- train %>%
  mutate(GarageCars = as.character(GarageCars)) %>%
  mutate(GarageCars = ifelse((GarageCars == "4" | GarageCars == "3"), "3+", GarageCars)) %>%
  mutate(GarageCars = as.factor(GarageCars))

test <- test %>%
  mutate(GarageCars = as.character(GarageCars)) %>%
  mutate(GarageCars = ifelse((GarageCars == "4" | GarageCars == "3"), "3+", GarageCars)) %>%
  mutate(GarageCars = as.factor(GarageCars))

str(train$GarageCars)

num_columns <- num_columns[!num_columns %in% c("GarageCars")]
cat_columns <- c(cat_columns, "GarageCars")

```

**GarageCars** & **SalePrice**:
```{r}
p <- ggplot(train, aes(x = GarageCars, y = SalePrice, fill = GarageCars)) +
     geom_boxplot()

ggplotly(p)

```

We remove **GarageCars** from our list of correlated numeric variables and create a scatterplot matrix:
```{r message=FALSE, warning=FALSE}

cor_10 <- cor_10[!cor_10 %in% c("GarageCars")]

p <- ggpairs(train[,c(cor_10[1:5],"SalePrice")])#,lower = list(continuous = wrap("points", alpha = 0.3,size = 0.5)))

ggplotly(p)


```


## Variable Importance {#varimp}
Lets now check variable importance for numeric features using a random forest model:
```{r message=FALSE, warning=FALSE}

# Set training control so that we only 1 run forest on the entire set of complete cases
data <- train[,c(num_columns)]

recipe <- recipe(data) %>%
  update_role(everything(), new_role = "predictor") %>%
  update_role(SalePrice, new_role = "outcome") %>%
  step_log(all_outcomes()) %>%
  step_knnimpute(all_predictors(),-all_nominal()) %>%
  step_normalize(all_predictors())

model <- daml_train(data, recipe,
                    model = "rf",
                    tracking = "mlflow",
                    tracking_uri = "http://localhost:5000",
                    mlflow_experiment = "house pricing",
                    save_model = T,
                    tunelen = 1)

varImp(model)

```



## Categorical Features {#catfeatures}
Lets take a look a categorical features and check for imbalances there:
```{r}
summary(train[,cat_columns])

```

Features like **Street**, **Utilities**, **Condition2**, **RoofMatl**, **Heating** are highly imbalanced. <br />
Other features also might have low frequency levels, but we can fix it in our **recipe** later. <br />
For now lets drop the above mentioned features from the dataset
```{r}
train <- train %>%
  select(-Street,-Utilities,-Condition2,-RoofMatl,-Heating)

test <- test %>%
  select(-Street,-Utilities,-Condition2,-RoofMatl,-Heating)

cat_columns <- cat_columns[!cat_columns %in% c("Street", "Utilities", "Condition2", "RoofMatl", "Heating")]

profile_missing(train)

```


Lets take a look at all features and their importance. <br /> 
First we will do target encoding for categorical features and see if they will good for modelling.
```{r message=FALSE, warning=FALSE}

recipe <- recipe(train) %>%
  update_role(everything(), new_role = "predictor") %>%
  update_role(SalePrice, new_role = "outcome") %>%
  update_role(Id, new_role = "ID variable") %>%
  step_log(all_outcomes()) %>%
  step_other(all_predictors(), -all_numeric(), threshold = 0.01) %>%
  step_novel(all_predictors(),-all_numeric(), new_level = "new") %>%
  step_lencode_mixed(all_predictors(), -all_numeric(), outcome = vars(SalePrice)) %>%
  step_knnimpute(all_predictors()) %>%
  step_corr(all_predictors()) %>%
  step_normalize(all_predictors())

model <- daml_train(train, recipe,
                    model = "rf",
                    tracking = "local",
                    run_name = "varimp",
                    tunelen = 1)

varImp(model)

```


Lets take a look at the **Neighborhood** and average prices of property per area. <br />
Location is one of the most importance price determinant together with the house size and quality of materials.
```{r}
summary(train$Neighborhood)
```


```{r include=FALSE}
train %>%
  dplyr::group_by(Neighborhood) %>%
  dplyr::summarise(SalePrice = mean(SalePrice)) %>%
  arrange(desc(SalePrice))

```


```{r}
  ggplot(train, aes(x = Neighborhood, y = SalePrice, fill = Neighborhood)) +
  geom_boxplot()

```

**OverallQual** is one of the most important features for this model
```{r}
  p <- ggplot(train, aes(x = OverallQual, y = SalePrice, fill = OverallQual)) +
  aes(group = OverallQual) +
  geom_boxplot()
  ggplotly(p)

```


Another interesting feature that detemines the location is **MSZoning**.
```{r}
  p <- ggplot(train, aes(x = MSZoning, y = SalePrice, fill = MSZoning)) +
  aes(group = MSZoning) +
  geom_boxplot()
  ggplotly(p)
```

Another interesting feature that detemines the location is **MSSubClass**.
```{r}
  p <- ggplot(train, aes(x = MSSubClass, y = SalePrice, fill = MSSubClass)) +
  aes(group = MSSubClass) +
  geom_boxplot()
  ggplotly(p)
```

## Model {#model}

Lets create the recipe:
```{r message=FALSE, warning=FALSE}

set.seed(123)

recipe <- recipe(train) %>%
  update_role(everything(), new_role = "predictor") %>%
  update_role(SalePrice, new_role = "outcome") %>%
  update_role(Id, new_role = "ID variable") %>%
  step_log(all_outcomes()) %>%
  step_other(all_predictors(), -all_numeric(), threshold = 0.01) %>%
  step_novel(all_predictors(),-all_numeric(), new_level = "new") %>%
  step_lencode_mixed(all_predictors(), -all_numeric(), outcome = vars(SalePrice)) %>%
  step_knnimpute(all_predictors()) %>%
  step_corr(all_predictors(), threshold = 0.8) %>%
  step_normalize(all_predictors())

```


Our first random forest model
```{r}

rf <- daml_train(train, recipe,
                  model = "rf",
                  feature_selection = "varimp",
                  max_features = 24,
                  p_subset = 0.4,
                  tracking = "mlflow",
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  tunelen = 5)

varImp(rf)
```

Now lets try our first submission!
```{r}
submission <- test %>%
  daml_predict(model = rf, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission, "rf_baseline.csv", row.names = F)

```

Our submission scored 0.14183 on a leaderboard!

Now lets try **gbm** instad of random forest
```{r message=FALSE, warning=FALSE}
gbm <- daml_train(train, recipe,
                  model = "gbm",
                  feature_selection = "varimp",
                  max_features = 24,
                  p_subset = 0.4,
                  tracking = "mlflow",
                  tracking_uri = "http://localhost:5000",
                  mlflow_experiment = "house pricing",
                  tunelen = 10)

varImp(gbm)

```


```{r message=FALSE, warning=FALSE}
submission <- test %>%
  daml_predict(model = gbm, pred_field = "SalePrice") %>%
  select(Id, SalePrice) %>%
  mutate(SalePrice = exp(SalePrice))

write.csv(submission, "gbm_baseline.csv", row.names = F)

```

Extraordinary - Our submission scored 0.12577 on a leaderboard! <br />
Now lets take a look at other ideas like **feature interactions**, **binning**, **PCA**, **Ensemble models**, dummy features for imputed records.

